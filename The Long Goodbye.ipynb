{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Long Goodbye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "fig,ax =  plt.subplots(1,2,figsize=(12,9))\n",
    "sns.kdeplot(train.x1,label='train_x1',ax=ax[0,0])\n",
    "sns.kdeplot(test.x1,label='test_x1',ax=ax[0,0])\n",
    "ax[0,0].set_title('x1')\n",
    "sns.kdeplot(train.x2,label='train_x2',ax=ax[0,1])\n",
    "sns.kdeplot(test.x2,label='test_x2',ax=ax[0,1])\n",
    "ax[0,1].set_title('x2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax =  plt.subplots(1,2,figsize=(12,9))\n",
    "sns.scatterplot(train.x1, train.y_, ax=ax[0,0])\n",
    "sns.scatterplot(train.x2, train.y_, ax=ax[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.T.isna(), cmap='Blues')\n",
    "ax.set_title('Fields with Missing Values', fontsize=16) #看特征同时为null的情况\n",
    "ax.set_ylabel('Year', fontsize=16)\n",
    "ax.set_xlabel('Feature', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df[['resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp']],corner=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#可能会发现新世界\n",
    "plt.scatter(x,y) \n",
    "alpha=0.01 #调整透明度\n",
    "s=1 #标点大小\n",
    "edgecolor='none'\n",
    "ax.set_xlim(xmin,xmax)\n",
    "ax.set_ylim(ymin,ymax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#降维并且可视化\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE()\n",
    "tsne.fit_transform(X) \n",
    "tsne = pd.DataFrame(tsne.embedding_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca=PCA(n_components=1)\n",
    "newData=pca.fit_transform(X)\n",
    "pca.n_components\n",
    "pca.explained_variance_ratio_\n",
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果KS统计量小或p值高，则分布相同(我们不能拒绝两个样本的分布相同的假设)。\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "def corr(train, test,cols):\n",
    "    # assuming first column is `class_name_id`\n",
    "\n",
    "\n",
    "    for class_name in cols:\n",
    "        # all correlations\n",
    "        print('\\n Class: %s' % class_name)\n",
    "\n",
    "        ks_stat, p_value = ks_2samp(train[class_name].values,\n",
    "                                    test[class_name].values)\n",
    "        print(' Kolmogorov-Smirnov test:    KS-stat = %.6f    p-value = %.3e\\n'\n",
    "              % (ks_stat, p_value))\n",
    "\n",
    "corr(train, test,cols=['year','month','day','hour','minute','weekday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "去掉空值多的(nan值一样的取unique值多的)，方差小的，相关性高的(train_reduced = VarianceThreshold(1.5).fit_transform(train))，特征重要性低的。  \n",
    "from feature_selector import FeatureSelector  \n",
    "https://nbviewer.jupyter.org/github/WillKoehrsen/feature-selector/blob/master/Feature%20Selector%20Usage.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "树模型\n",
    "空值替换为-999和不替换两种方法比较一下\n",
    "LGB类别变量 label encoding(作为数字)和作为category两种方法比较一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding\n",
    "\n",
    "# use target encoding to encode two categorical features\n",
    "enc = TargetEncoder(cols=['CHAS', 'RAD'])\n",
    "\n",
    "# transform the datasets\n",
    "training_numeric_dataset = enc.fit_transform(X_train, y_train)\n",
    "testing_numeric_dataset = enc.transform(X_test)\n",
    "\n",
    "# encoding function\n",
    "\n",
    "# frequency encode\n",
    "def encode_FE(df1, df2, cols):\n",
    "    for col in cols:\n",
    "        df = pd.concat([df1[col], df2[col]])\n",
    "        vc = df.value_counts(dropna=True, normalize=True).to_dict()\n",
    "        vc[-1] = -1\n",
    "        nm = col + \"FE\"\n",
    "        df1[nm] = df1[col].map(vc)\n",
    "        df1[nm] = df1[nm].astype(\"float32\")\n",
    "        df2[nm] = df2[col].map(vc)\n",
    "        df2[nm] = df2[nm].astype(\"float32\")\n",
    "        print(col)\n",
    "\n",
    "\n",
    "# label encode\n",
    "def encode_LE(col, train=X_train, test=X_test, verbose=True):\n",
    "    df_comb = pd.concat([train[col], test[col]], axis=0)\n",
    "    df_comb, _ = pd.factorize(df_comb[col])\n",
    "    nm = col\n",
    "    if df_comb.max() > 32000:\n",
    "        train[nm] = df_comb[0: len(train)].astype(\"float32\")\n",
    "        test[nm] = df_comb[len(train):].astype(\"float32\")\n",
    "    else:\n",
    "        train[nm] = df_comb[0: len(train)].astype(\"float16\")\n",
    "        test[nm] = df_comb[len(train):].astype(\"float16\")\n",
    "    del df_comb\n",
    "    gc.collect()\n",
    "    if verbose:\n",
    "        print(col)\n",
    "\n",
    "\n",
    "def encode_AG(main_columns, uids, aggregations=[\"mean\"], df_train=X_train, df_test=X_test, fillna=True, usena=False):\n",
    "    for main_column in main_columns:\n",
    "        for col in uids:\n",
    "            for agg_type in aggregations:\n",
    "                new_column = main_column + \"_\" + col + \"_\" + agg_type\n",
    "                temp_df = pd.concat([df_train[[col, main_column]], df_test[[col, main_column]]])\n",
    "                if usena:\n",
    "                    temp_df.loc[temp_df[main_column] == -1, main_column] = np.nan\n",
    "\n",
    "                #求每个uid下，该col的均值或标准差\n",
    "                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n",
    "                    columns={agg_type: new_column})\n",
    "                #将uid设成index\n",
    "                temp_df.index = list(temp_df[col])\n",
    "                temp_df = temp_df[new_column].to_dict()\n",
    "                #temp_df是一个映射字典\n",
    "                df_train[new_column] = df_train[col].map(temp_df).astype(\"float32\")\n",
    "                df_test[new_column] = df_test[col].map(temp_df).astype(\"float32\")\n",
    "                if fillna:\n",
    "                    df_train[new_column].fillna(-1, inplace=True)\n",
    "                    df_test[new_column].fillna(-1, inplace=True)\n",
    "                print(new_column)\n",
    "\n",
    "\n",
    "# COMBINE FEATURES交叉特征\n",
    "def encode_CB(col1, col2, df1=X_train, df2=X_test):\n",
    "    nm = col1 + '_' + col2\n",
    "    df1[nm] = df1[col1].astype(str) + '_' + df1[col2].astype(str)\n",
    "    df2[nm] = df2[col1].astype(str) + '_' + df2[col2].astype(str)\n",
    "    encode_LE(nm, verbose=False)\n",
    "    print(nm, ', ', end='')\n",
    "\n",
    "\n",
    "# GROUP AGGREGATION NUNIQUE\n",
    "def encode_AG2(main_columns, uids, train_df=X_train, test_df=X_test):\n",
    "    for main_column in main_columns:\n",
    "        for col in uids:\n",
    "            comb = pd.concat([train_df[[col] + [main_column]], test_df[[col] + [main_column]]], axis=0)\n",
    "            mp = comb.groupby(col)[main_column].agg(['nunique'])['nunique'].to_dict()\n",
    "            train_df[col + '_' + main_column + '_ct'] = train_df[col].map(mp).astype('float32')\n",
    "            test_df[col + '_' + main_column + '_ct'] = test_df[col].map(mp).astype('float32')\n",
    "            print(col + '_' + main_column + '_ct, ', end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN\n",
    "model = Sequential()  \n",
    "model.add(Dense(32, input_dim=4, init='uniform', activation='relu'))\n",
    "model.add(Dropout(0.3))  \n",
    "model.add(Dense(16, init='uniform', activation='relu'))\n",
    "model.add(Dropout(0.3)) \n",
    "model.add(Dense(8, init='uniform', activation='relu')) \n",
    "model.add(Dropout(0.3)) \n",
    "model.add(Dense(2, init='uniform', activation='softmax')) \n",
    "opt = optimizers.Nadam(lr=0.0001)\n",
    " \n",
    "model.compile(loss='binary_crossentropy', optimizer=opt,metrics = ['accuracy']) \n",
    "model.fit(X_train, y_train, nb_epoch=3000, batch_size=40,verbose=1, validation_data=(X_test,y_test))#不需要class_weight=cw\n",
    "predictions = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LGB\n",
    "d_train = lgb.Dataset(train[features], train['y_'])\n",
    "clf = lgb.train(trial.params,d_train) #after optuna fine tune\n",
    "all_pred = clf.predict(test[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGB\n",
    "d_train = xgb.DMatrix(train[features], train['y_'])\n",
    "clf = xgb.train(trial.params, d_train, obj=huber_approx_obj)\n",
    "all_pred = clf.predict(xgb.DMatrix(test[features]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optuna \n",
    "#目标函数(objective)，单次试验(trial)，和研究(study). 其中 objective 负责定义待优化函数并指定参/超参数数范围，trial 对应着 objective 的单次执行，而 study 则负责管理优化，决定优化的方式，总试验的次数、试验结果的记录等功能。\n",
    "\n",
    "import optuna\n",
    "def objective(trial):\n",
    "\n",
    "    # Optuna suggest params\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 100),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_uniform('learning_rate', 0.01, 0.10),\n",
    "        'objective': 'huber',\n",
    "        'lambda': trial.suggest_uniform('lambda', 0, 1),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0, 1),\n",
    "        'bagging_freq': 2\n",
    "    }\n",
    "\n",
    "    score=[]\n",
    "    tscv = TimeSeriesSplit()\n",
    "    for train_index, val_index in tscv.split(train):\n",
    "        X_train, X_val = train.loc[train_index,features], train.loc[val_index,features]\n",
    "        y_train, y_val = train.loc[train_index,'y_'], train.loc[val_index,'y_']\n",
    "        d_train = lgb.Dataset(X_train, y_train)\n",
    "        d_val = lgb.Dataset(X_val, y_val)\n",
    "        clf = lgb.train(params,d_train)\n",
    "        val_pred = clf.predict(X_val)\n",
    "        score.append(mean_absolute_error(val_pred,y_val))\n",
    "    return np.mean(score)\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=60)\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果不指定 params 也是可以的，optuna 将画出所有的参数之间的关系，在这里两者等价。\n",
    "optuna.visualization.plot_contour(study, params=['x', 'y'])\n",
    "optuna.visualization.plot_contour(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里提出一种简单的初始化调参方式：1、使用不同的初始化方法，xavier，glorot，he_uniform等，可以把torch或tf.keras支持的各类初始化方式作为超参数；2、不同的超参数多次进行随机初始化；3、每一个初始化之后的网络都直接进行预测，计算评价指标通过这种简单的方式，可以在相对合理的时间里找到最优的初始化方法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "伪标签 详见知乎：求是汪\n",
    "选概率为0.99 0.01的 硬标签\n",
    "也有软标签\n",
    "回归问题 计算两次模型误差最小的\n",
    "半监督学习：思想很简单，对于标注好的样本一样用交叉熵作为损失函数，对于没标注的样本，则用Consistency Loss。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "拒绝演绎\n",
    "样本偏差对信贷风控模型的影响？\n",
    "在头部优质客群上所建立的模型，在全量客群上效果自然会衰减。这解释了用单一机构的Y所建立的KGB模型，在其他机构上可能完全失效的现象。如果你测试过三方数据商提供的评分产品，你就会有更深的体会。\n",
    "推荐：利用两阶段双评分卡来进行拒绝推断探索。全样本预测通过拒绝score1，通过样本预测是否违约score2。score2=f(score1)。然后得到拒绝样本的score2'。利用拒绝推断重新训练是否违约模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每一个fold(5 fold)的test set做预测最后拼起来变成oof，然后metrics(train.y,oof.prediction)。否则就是每个fold自己的metrics了。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2个模型ensemble，确定每个模型的权重w，使w加权的oof的metrics最大  \n",
    "\\>2个Start with the one model that has highest CV score. Next iterate through all your additional models and find the one model that combines with the first model to generate the highest two model ensemble CV score. Then search for the best third model. Repeat until ensemble CV does not increase anymore.  \n",
    "e.g. model_ensemble_forward_selection.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "frequency encoding很重要，做EDA的时候也可以看，或者直接把它们当特征扔进去"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "样本极度不平衡时，ROC曲线会失真，要看PR曲线。\n",
    "类别不平衡：我会比较倾向使用改造函数函数来调接样本不平衡的问题，因为无论是过采样或者欠采样，都会导致样本噪声这个问题，而改造损失函数，如调整不同样本的权重，采用Focal Loss等等函数都能有限减少不平衡的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感觉树模型的时序特征也要平衡(去除趋势)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "McNemar test statistic可以用来比较两个模型表现。一堆比较模型表现的统计量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "earlystop选最好的round可能过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "样本还是要shuffle，不然一个batch一样可能会把模型打偏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PSI\n",
    "def psi_for_continue_var(expected_array, actual_array, bins=10, bucket_type='bins', detail=False, save_file_path=None):\n",
    "    '''\n",
    "    ----------------------------------------------------------------------\n",
    "    功能: 计算连续型变量的群体性稳定性指标（population stability index ,PSI）\n",
    "    ----------------------------------------------------------------------\n",
    "    :param expected_array: numpy array of original values，基准组\n",
    "    :param actual_array: numpy array of new values, same size as expected，比较组\n",
    "    :param bins: number of percentile ranges to bucket the values into，分箱数, 默认为10\n",
    "    :param bucket_type: string, 分箱模式，'bins'为等距均分，'quantiles'为按等频分箱\n",
    "    :param detail: bool, 取值为True时输出psi计算的完整表格, 否则只输出最终的psi值\n",
    "    :param save_file_path: string, csv文件保存路径. 默认值=None. 只有当detail=Ture时才生效.\n",
    "    ----------------------------------------------------------------------\n",
    "    :return psi_value: \n",
    "            当detail=False时, 类型float, 输出最终psi计算值;\n",
    "            当detail=True时, 类型pd.DataFrame, 输出psi计算的完整表格。最终psi计算值 = list(psi_value['psi'])[-1]\n",
    "    ----------------------------------------------------------------------\n",
    "    示例：\n",
    "    >>> psi_for_continue_var(expected_array=df['score'][:400],\n",
    "                             actual_array=df['score'][401:], \n",
    "                             bins=5, bucket_type='bins', detail=0)\n",
    "    >>> 0.0059132756739701245\n",
    "    ------------\n",
    "    >>> psi_for_continue_var(expected_array=df['score'][:400],\n",
    "                             actual_array=df['score'][401:], \n",
    "                             bins=5, bucket_type='bins', detail=1)\n",
    "    >>>\n",
    "    \tscore_range\texpecteds\texpected(%)\tactucalsactucal(%)ac - ex(%)ln(ac/ex)psi\tmax\n",
    "    0\t[0.021,0.2095]\t120.0\t30.00\t152.0\t31.02\t1.02\t0.033434\t0.000341\t\n",
    "    1\t(0.2095,0.398]\t117.0\t29.25\t140.0\t28.57\t-0.68\t-0.023522\t0.000159\t\n",
    "    2\t(0.398,0.5865]\t81.0\t20.25\t94.0\t19.18\t-1.07\t-0.054284\t0.000577\t<<<<<<<\n",
    "    3\t(0.5865,0.7751]\t44.0\t11.00\t55.0\t11.22\t0.22\t0.019801\t0.000045\t\n",
    "    4\t(0.7751,0.9636]\t38.0\t9.50\t48.0\t9.80\t0.30\t0.031087\t0.000091\t\n",
    "    5\t>>> summary\t400.0\t100.00\t489.0\t100.00\tNaN\tNaN\t0.001214\t<<< result\n",
    "    ----------------------------------------------------------------------\n",
    "    知识:\n",
    "    公式： psi = sum(（实际占比-预期占比）* ln(实际占比/预期占比))\n",
    "    一般认为psi小于0.1时候变量稳定性很高，0.1-0.25一般，大于0.25变量稳定性差，建议重做。\n",
    "    相对于变量分布(EDD)而言, psi是一个宏观指标, 无法解释两个分布不一致的原因。但可以通过观察每个分箱的sub_psi来判断。\n",
    "    ----------------------------------------------------------------------\n",
    "    '''\n",
    "    import math\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    expected_array = pd.Series(expected_array).dropna()\n",
    "    actual_array = pd.Series(actual_array).dropna()\n",
    "    if isinstance(list(expected_array)[0], str) or isinstance(list(actual_array)[0], str):\n",
    "        raise Exception(\"输入数据expected_array只能是数值型, 不能为string类型\")\n",
    "        \n",
    "    \"\"\"step1: 确定分箱间隔\"\"\"\n",
    "    def scale_range(input_array, scaled_min, scaled_max):\n",
    "        '''\n",
    "        ----------------------------------------------------------------------\n",
    "        功能: 对input_array线性放缩至[scaled_min, scaled_max]\n",
    "        ----------------------------------------------------------------------\n",
    "        :param input_array: numpy array of original values, 需放缩的原始数列\n",
    "        :param scaled_min: float, 放缩后的最小值\n",
    "        :param scaled_min: float, 放缩后的最大值\n",
    "        ----------------------------------------------------------------------\n",
    "        :return input_array: numpy array of original values, 放缩后的数列\n",
    "        ----------------------------------------------------------------------\n",
    "        '''\n",
    "        input_array += -np.min(input_array) # 此时最小值放缩到0\n",
    "        if scaled_max == scaled_min:\n",
    "            raise Exception('放缩后的数列scaled_min = scaled_min, 值为{}, 请检查expected_array数值！'.format(scaled_max))\n",
    "        scaled_slope = np.max(input_array) * 1.0 / (scaled_max - scaled_min)\n",
    "        input_array /= scaled_slope\n",
    "        input_array += scaled_min\n",
    "        return input_array\n",
    "    \n",
    "    # 异常处理，所有取值都相同时, 说明该变量是常量, 返回999999\n",
    "    if np.min(expected_array) == np.max(expected_array):\n",
    "        return 999999\n",
    "    \n",
    "    breakpoints = np.arange(0, bins + 1) / (bins) * 100 # 等距分箱百分比\n",
    "    if 'bins' == bucket_type:        # 等距分箱\n",
    "        breakpoints = scale_range(breakpoints, np.min(expected_array), np.max(expected_array))\n",
    "    elif 'quantiles' == bucket_type: # 等频分箱\n",
    "        breakpoints = np.stack([np.percentile(expected_array, b) for b in breakpoints])\n",
    " \n",
    "    \"\"\"step2: 统计区间内样本占比\"\"\"\n",
    "    def generate_counts(arr, breakpoints):\n",
    "        '''\n",
    "        ----------------------------------------------------------------------\n",
    "        功能: Generates counts for each bucket by using the bucket values \n",
    "        ----------------------------------------------------------------------\n",
    "        :param arr: ndarray of actual values\n",
    "        :param breakpoints: list of bucket values\n",
    "        ----------------------------------------------------------------------\n",
    "        :return cnt_array: counts for elements in each bucket, length of breakpoints array minus one\n",
    "        :return score_range_array: 分箱区间\n",
    "        ----------------------------------------------------------------------\n",
    "        '''\n",
    "        def count_in_range(arr, low, high, start):\n",
    "            '''\n",
    "            ----------------------------------------------------------------------\n",
    "            功能: 统计给定区间内的样本数(Counts elements in array between low and high values)\n",
    "            ----------------------------------------------------------------------\n",
    "            :param arr: ndarray of actual values\n",
    "            :param low: float, 左边界\n",
    "            :param high: float, 右边界\n",
    "            :param start: bool, 取值为Ture时，区间闭合方式[low, high],否则为(low, high]\n",
    "            ----------------------------------------------------------------------\n",
    "            :return cnt_in_range: int, 给定区间内的样本数\n",
    "            ----------------------------------------------------------------------\n",
    "            '''\n",
    "            if start:\n",
    "                cnt_in_range = len(np.where(np.logical_and(arr >= low, arr <= high))[0])\n",
    "            else:\n",
    "                cnt_in_range = len(np.where(np.logical_and(arr > low, arr <= high))[0])\n",
    "            return cnt_in_range\n",
    " \n",
    "        cnt_array = np.zeros(len(breakpoints) - 1)\n",
    "        score_range_array = [''] * (len(breakpoints) - 1)\n",
    "        for i in range(1, len(breakpoints)):\n",
    "            cnt_array[i-1] = count_in_range(arr, breakpoints[i-1], breakpoints[i], i==1)\n",
    "            if 1 == i:\n",
    "                score_range_array[i-1] = '[' + str(round(breakpoints[i-1], 4)) + ',' + str(round(breakpoints[i], 4)) + ']'\n",
    "            else:   \n",
    "                score_range_array[i-1] = '(' + str(round(breakpoints[i-1], 4)) + ',' + str(round(breakpoints[i], 4)) + ']'\n",
    "                                                                                \n",
    "        return (cnt_array, score_range_array)\n",
    " \n",
    "    expected_cnt = generate_counts(expected_array, breakpoints)[0]\n",
    "    expected_percents = expected_cnt / len(expected_array)\n",
    "    actual_cnt = generate_counts(actual_array, breakpoints)[0]\n",
    "    actual_percents = actual_cnt / len(actual_array)\n",
    "    delta_percents = actual_percents - expected_percents\n",
    "    score_range_array = generate_counts(expected_array, breakpoints)[1]\n",
    "                                                                               \n",
    "    \"\"\"step3: 区间放缩\"\"\"\n",
    "    def sub_psi(e_perc, a_perc):\n",
    "        '''\n",
    "        ----------------------------------------------------------------------\n",
    "        功能: 计算单个分箱内的psi值。Calculate the actual PSI value from comparing the values.\n",
    "             Update the actual value to a very small number if equal to zero\n",
    "        ----------------------------------------------------------------------\n",
    "        :param e_perc: float, 期望占比\n",
    "        :param a_perc: float, 实际占比\n",
    "        ----------------------------------------------------------------------\n",
    "        :return value: float, 单个分箱内的psi值\n",
    "        ----------------------------------------------------------------------\n",
    "        '''\n",
    "        if a_perc == 0: # 实际占比\n",
    "            a_perc = 0.001\n",
    "        if e_perc == 0: # 期望占比\n",
    "            e_perc = 0.001\n",
    "        value = (e_perc - a_perc) * np.log(e_perc * 1.0 / a_perc)\n",
    "        return value\n",
    "    \n",
    "    \"\"\"step4: 得到最终稳定性指标\"\"\"\n",
    "    sub_psi_array = [sub_psi(expected_percents[i], actual_percents[i]) for i in range(0, len(expected_percents))]\n",
    "    if detail:\n",
    "        psi_value = pd.DataFrame()\n",
    "        psi_value['score_range'] = score_range_array\n",
    "        psi_value['expecteds'] = expected_cnt\n",
    "        psi_value['expected(%)'] = expected_percents * 100\n",
    "        psi_value['actucals'] = actual_cnt\n",
    "        psi_value['actucal(%)'] = actual_percents * 100\n",
    "        psi_value['ac - ex(%)'] = delta_percents * 100\n",
    "        psi_value['actucal(%)'] = psi_value['actucal(%)'].apply(lambda x: round(x, 2))\n",
    "        psi_value['ac - ex(%)'] = psi_value['ac - ex(%)'].apply(lambda x: round(x, 2))\n",
    "        psi_value['ln(ac/ex)'] = psi_value.apply(lambda row: np.log((row['actucal(%)'] + 0.001) \\\n",
    "                                                                  / (row['expected(%)'] + 0.001)), axis=1)\n",
    "        psi_value['psi'] = sub_psi_array\n",
    "        flag = lambda x: '<<<<<<<' if x == psi_value.psi.max() else ''\n",
    "        psi_value['max'] = psi_value.psi.apply(flag)\n",
    "        psi_value = psi_value.append([{'score_range':'>>> summary', \n",
    "                                       'expecteds': sum(expected_cnt),\n",
    "                                       'expected(%)':100, \n",
    "                                       'actucals': sum(actual_cnt),\n",
    "                                       'actucal(%)':100, \n",
    "                                       'ac - ex(%)': np.nan,\n",
    "                                       'ln(ac/ex)': np.nan,\n",
    "                                       'psi': np.sum(sub_psi_array),\n",
    "                                       'max':'<<< result'}], ignore_index=True)\n",
    "        if save_file_path:\n",
    "            if not isinstance(save_file_path, str):\n",
    "                raise Exception('参数save_file_path类型必须是str, 同时注意csv文件后缀!')\n",
    "            elif not save_file_path.endswith('.csv'):\n",
    "                raise Exception('参数save_file_path不是csv文件后缀，请检查!')\n",
    "            psi_value.to_csv(save_file_path, encoding='utf-8', index=1)\n",
    "    else:\n",
    "        psi_value = np.sum(sub_psi_array)\n",
    " \n",
    "    return psi_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PurgedGroupTimeSeriesSplit\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
    "    Allows for a gap in groups to avoid potentially leaking info from\n",
    "    train into test if the model has windowed or lag features.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_group_size : int, default=Inf\n",
    "        Maximum group size for a single training set.\n",
    "    group_gap : int, default=None\n",
    "        Gap between train and test\n",
    "    max_test_group_size : int, default=Inf\n",
    "        We discard this number of groups from the end of each train split\n",
    "    \"\"\"\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.group_gap\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                \n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "\n",
    " \n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "\n",
    "            test_array  = test_array[group_gap:]\n",
    "            \n",
    "            \n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "                    \n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]\n",
    "            \n",
    "cv = PurgedGroupTimeSeriesSplit(n_splits = n_splits, group_gap = group_gap)\n",
    "for i, (train, valid) in enumerate(cv.split(X=X, y=y, groups=group)):\n",
    "    data[train]\n",
    "    data[valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T11:10:56.393424Z",
     "start_time": "2021-04-04T11:10:56.388653Z"
    }
   },
   "source": [
    "## Other tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING,\n",
    "                    filename='./log.txt',   \n",
    "                    filemode='w',  \n",
    "                    format='%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s')\n",
    "# use logging\n",
    "logging.info('this is a loggging info message')\n",
    "logging.debug('this is a loggging debug message')\n",
    "logging.warning('this is loggging a warning message')\n",
    "logging.error('this is an loggging error message')\n",
    "logging.critical('this is a loggging critical message')\n",
    "\n",
    "try:\n",
    "    fun()\n",
    "except Exception as e:\n",
    "    logging.error('something wrong',exc_info=True)\n",
    "logging.info('start')\n",
    "fun()\n",
    "logging.info('end')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多进程：multiprocess  \n",
    "每个进程拷贝自己的变量  \n",
    "多线程：threading  \n",
    "变量共享  \n",
    "并且线程由于GIL并不能执行多核任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df,inplace=True):\n",
    "    if not inplace:\n",
    "        df=df.copy()\n",
    "    start_mem=df.memory_usage().sum()/1024**2\n",
    "    print('Memory usage of dataframe is {:.2f}MB'.format(start_mem))\n",
    "    for col in df.columns:\n",
    "        col_type=df[col].dtype\n",
    "        if col_type!=object:\n",
    "            c_min=df[col].min()\n",
    "            c_max=df[col].max()\n",
    "            if str(col_type)[:3]=='int':\n",
    "                if c_min>np.iinfo(np.int8).min and c_max<np.iinfo(np.int8).max:\n",
    "                    df[col]=df[col].astype(np.int8)\n",
    "                elif c_min>np.iinfo(np.int16).min and c_max<np.iinfo(np.int16).max:\n",
    "                    df[col]=df[col].astype(np.int16)    \n",
    "                elif c_min>np.iinfo(np.int32).min and c_max<np.iinfo(np.int32).max:\n",
    "                    df[col]=df[col].astype(np.int32)\n",
    "                elif c_min>np.iinfo(np.int64).min and c_max<np.iinfo(np.int64).max:\n",
    "                    df[col]=df[col].astype(np.int64)                    \n",
    "            else:\n",
    "                if c_min>np.finfo(np.float16).min and c_max<np.finfo(np.float16).max:\n",
    "                    df[col]=df[col].astype(np.float16)                \n",
    "                elif c_min>np.finfo(np.float32).min and c_max<np.finfo(np.float32).max:\n",
    "                    df[col]=df[col].astype(np.float32) \n",
    "                else:\n",
    "                    df[col]=df[col].astype(np.float64) \n",
    "        else:\n",
    "            df[col]=df[col].astype('category') \n",
    "    end_mem=df.memory_usage().sum()/1024**2\n",
    "    print('Memory usage after optimization is {:.2f}MB'.format(end_mem))   \n",
    "    print('Decreased by {:.1f}%'.format(100*(start_mem-end_mem)/start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "高性能的pandas：eval()和query()  \n",
    "对于过滤的操作，可以使用query()方法。  \n",
    "Numexpr程序库可以让你在不为中间过程分配全部内存的前提下，完成元素到元素的复合代数式运算。Pandas的eval()和query()工具就是基于Numexpr实现的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Git\n",
    "- git clone\n",
    "- git branch --list\n",
    "- git branch -r 查看远程分支\n",
    "- git checkout 本地分支\n",
    "- git status\n",
    "- git add/ git add --all(这样才会把删除操作加进来)\n",
    "- git commit -m 'msg'\n",
    "- git push/ git push origin 远程分支名\n",
    "- git pull 仅针对所在的分支\n",
    "    - 防止本地修改被override,1). git stash(保存工作现场),git stash pop(恢复工作现场)\n",
    "    2). git add, git commit.\n",
    "    - 若有冲突，解决完冲突后，还需要add，commit\n",
    "    - 看到有++--说明文件被真正改动\n",
    "- git merge dev(把dev合并到当前分支)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
